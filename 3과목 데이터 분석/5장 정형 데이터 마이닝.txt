3과목 데이터 분석
5장. 정형 데이터 마이닝


>>> [ 데이터 마이닝 개요 ] 8:37

* 데이터 마이닝 
	- 기업이 보유하고 있는 일일 거래 데이터, 고객 데이터, 상품 데이터 혹은 각종 마케팅 활동에 있어서의 고객 반응 데이터 등과 이외의 외부 데이터를 포함하는 모든 사용가능한 원천 데이터를 기반으로 감춰진 지식, 기대하지 못했던 경향 또는 새로운 규칙 등을 발견하고 이를 실제 비즈니스 의사결정 등에 유용한 정보로 활용하는 일련의 작업
	- 다양한 수리 알고리즘을 이용해 DB의 데이터로부터 의미있는 정보를 찾아내는 방법을 통칭

* 데이터 마이닝 5단계
	1) 목적 정의
		- 데이터 마이닝 도입 목적을 명확하게 함
	2) 데이터 준비
		- 데이터 정제를 통해 데이터의 품질 확보까지 포함
		- 필요시 데이터 양 충분하게 확보
	3) 데이터 가공
		- 목적 변수를 정의하고, 필요한 데이터를 데이터 마이닝 소프트웨어에 적용할 수 있게 가공 및 준비하는 단계	
		- 충분한 CPU와 메모리, 디스크 공간 등 개별환경 구축이 선행
	4) 데이터 마이닝 기법 적용
		- 모델을 목적에 맞게 선택하고 소프트웨어를 사용하는데 필요한 값 지정
	5) 검증
		- 결과에 대한 검증 시행

* 대표적 데이터 마이닝 기법
	1) 분류(Classification)
		- 새롭게 나타난 현상을 검토하여 기존의 분류, 정의된 집합에 배정하는 것
		- 의자결정 나무, memory-based reasoning
	2) 추정(Estimation)
		- 주어진 입력 데이터를 사용하여 알려지지 않은 결과의 값을 추정하는 것
		- 연속된 변수의 값을 추정, 신경망 모형
	3) 연관분석(Association Analysis)
		- '같이 팔리는 물건' 같이 아이템의 연관성을 파악하는 분석
		- 카탈로그 배열 및 교차판매, 공격적 판촉행사 등의 마케팅 계획
	4) 예측(Prediction)
		- 미래에 대한 것을 예측, 추정하는 것을 제외하면 분류나 추정과 동일한 의미
		- 장바구니 분석, 의사결정나무, 신경망 모형
	5) 군집(Clustering)
		- 미리 정의된 기준이나 예시에 의해서가 아닌 레코드 자체가 가진 다른 레코드와의 유사성에 의해 그룹화되고 이질성에 의해 세분화됨
		- 데이터 마이닝이나 모델링의 준비 단계로서 사용됨
	6) 기술(Description) 
		- 데이터가 가진 특징 및 의미를 단순하게 설명하는 것
		- 데이터가 암시하는 바에 대해 설명 및 그에 대한 답을 찾아 낼 수 있어야 함




>>> [ 데이터 분석 순서, ML 알고리즘 종류 ] 12:33

* 데이터 분석 순서
	1) 분석용 데이터 준비
	2) 탐색적 분석 데이터 전처리
		- 결측값, 이상값 처리, 특성조작, 데이터 차원 축소
	3) 모델링
		- 회귀분석, 분류분석, 군집분석, 연관분석
	4) 모델 평가 및 검증
		- 결정계수(R^2), F통계량, t값, ROC Curve, 오분류표, 실루엣, DI
	5) 모델 적응 운영 방안 수립

* Machine Learning Algorithms [ 머신러닝 알고리즘 ; ML ]
	- 지도학습(Supervised) : 입력, 출력 모두 알려줌
		=> 회귀(연속), 분류(범주)
	- 비지도학습(Unsupervised) : 입력만 알려줌
		=> 연관분석(연속), 차원축소(연속), 군집(범주)

/var/folders/gg/y9yn2n_x0rn4qg3nd8jlnn000000gn/T/TemporaryItems/NSIRD_screencaptureui_k3xKmx/스크린샷 2023-05-13 오전 3.47.42.png

* 분류분석의 종류
	1) 로지스틱 회귀
	2) 의사결정 나무
	3) 앙상블
	4) 신경망 모형
	5) kNN, 베이즈분류 모형, SVM(서포트벡터기계), 유전자 알고리즘




>>> [ 분류 알고리즘 - 로지스틱 회귀 분석 ] 16:49

* 로지스틱 회귀분석
	- 독립변수는 연속형, 종속변수가 범주형인 경우 적용되는 회귀분석 모형
	- 종속변수가 성공/실패, 사망/생존과 같이 이항변수(0,1)로 되어 있을 때 종속변수와 독립변수 간의 관계식을 이용하여 두 집단 또는 그 이상의 집단을 분류하고자 할 때 사용되는 분석기법
	- 종속변수를 전체 실수 범위로 확장하여 분석하고, sigmoid 함수를 사용해 연속형 0~1값으로 변경

---------------------------------------------------------------
		| 일반 선형 회귀분석	| 로지스틱 회귀분석
---------------------------------------------------------------
종속변수		| 연속형 변수		| 이산형(범주형) 변수
---------------------------------------------------------------
모형 탐색 방법	| 최소자승(LSM,최소제곱)	| 최대우도법(MLE), 가중최소자승법
---------------------------------------------------------------
모형 검정		| F-test, T-test	| x^2test(카이제곱)
---------------------------------------------------------------

-> 선형회귀 
	- x값의 변화에 따른 y값의 변화를 알아냄
	- x가 1증가할 때, y는 회귀계수 만큼 증가함

-> 로지스틱 회귀
	- x값에 따른 y값의 변화량 문제가 아님!
	- 회귀계수를 해석할 때 문제가 생김

Q. 로지스틱 회귀모형에서 exp(x1)의 의미는 나머지 변수가 주어질 때 x1이 한 단위 증가할 때마다 성공(Y=1)의 "odds(오즈)"가 몇 배 증가하는지를 나타낸다.
Q. 종속변수가 성공 또는 실패인 이항변수로 되어 있을 떄 종속변수와 독립변수 간의 관계식을 이용하여 두 집단 또는 그 이상의 집단을 분류하고자 할 때 사용되는 분석기법은 "로지스틱 회귀분석"이다

* 다중회귀분석 : 반응변수가 연속 형인 경우 적용되는 회귀분석 모형
* 교차분석 : 두 범주형 변수간 연관 관계를 분석하는 것으로 카이스퀘어 검정이라고 함
* 더미 회귀분석 : 독립변수에 명목척도나 서열척도가 있는 경우 이를 더미화하여 회귀를 분석하는 것




>>> [ 분류 알고리즘 - 의사결정 나무(Decision Tree) ] 18:21

* 의사결정 규칙을 나무 구조로 나타내 전체 자료를 몇 개의 소집단으로 분류하거나 예측을 수행하는 분석방법
* 분석과적이 직관적이고 이해하기 쉬움

* 뿌리마디(root node)
* 자식마디(child node)
* 부모마디(parent node)
* 최종마디(terminal node)
* 중간마디(internal node)
* 가지(branch)
* 깊이(depth)

* 특징
	- 목적은 새로운 데이터를 분류하거나 값을 예측하는 것이다.
	- 분리 변수 P차원 공간에 대한 현재 분할은 이전 분할에 영향을 받는다
	- 부모마디보다 자식마디의 순수도가 증가하도록 분류나무를 형성해 나간다(불순도 감소)

* 독립변수(설명변수, 예측변수, Feature)
* 종속변수(목표변수, 반응변수, Label)

* 종류
	- 목표변수가 이산형인 경우의 분류나무(classification tree)
	- 목표변수가 연속형인 경우의 회귀나무

* 장점 
	- 구조가 단순하여 해석이 용이함
	- 비모수적 모형으로 선형성, 정규성, 등분산성 등의 수학적 가정이 불필요함
	- 범주형(이산형)과 수치형(연속형) 변수를 모두 사용할 수 있음

* 단점 
	- 분류 기준값의 경계선 부근의 자료 값에 대해서는 오차가 큼(비연속성)
	- 로지스틱 회귀와 같이 각 예측변수의 효과를 파악하기 어려움
	- 새로운 자료에 대한 예측이 불안정할 수 있음

* 의사결정나무의 결정 규칙
	- 분리 기준
		1) Split criterion, 새로운 가지를 만드는 기준을 어떻게 정해야 할까?
		2) 순수도가 높아지는 방향으로 분리
		3) 불확실성이 낮아지는 방향

	- 정지 규칙(Stopping rule)
		1) 더 이상 분리가 일어나지 않고 현재의 마디가 최종마디가 되도록 하는 규칙
		2) '불순도 감소량'이 아주 작을 때 정지함
	- 가지치기 규칙(Pruning rule)
		1) 어느 가지를 쳐내야 예측력이 좋은 나무가 될까?
		2) 최종 노드가 너무 많으면 Overfitting 가능성이 커짐, 이를 해결하기 위해 사용
		3) 가지치지 규칙은 별도 규칙을 제공하거나 경험에 의해 실행할 수 있음
		4) 가지치기의 비용함수를 최소로 하는 분기를 찾아내도록 학습
		5) Information Gain이란 어떤 속성을 선택함으로 인해 데이터를 더 잘 구분하게 되는 것을 의미함(불확실성 감소)

* 불순도 측정 지표
* 목표변수가 범주형일 떄 사용하는 지표(분류에서 사용)
	1) 지니지수 Gini(T) 1-Σ(각 범주별수/전체수)^2
		- 불순도 측정 지표, 값이 작을수록 순수도가 높음(분류가 잘됨)
		- 가장 작은 값을 갖는 예측변수와 이때 최적 분리에 의해 자식 마디 형성
		=> min( 색으로 구분, 모양으로 구분 )
	2) 엔트로피 지수 Entropy(T)
		- 불순도 측정 지표, 가장 작은 값을 갖는 방법 선택
	3) 카이제곱 통계량의 유의 확률(p-value)
	
* 의사결정나무를 위한 알고리즘
	- 의사결정나무를 위한 알고리즘은 CHAID, CART, ID2, C5.0, C4.5가 있으며 하향식 접근 방법을 이용한다.

* 알고리즘 별 분리, 정지 기준변수 선택법
-----------------------------------------------------------------
알고리즘		| 이산형 목표변수(분류나무)	| 연속형 목표변수(회귀나무)
-----------------------------------------------------------------
CART		| 지니지수			| 분산 감소량
-----------------------------------------------------------------
C5.0		| 엔트로피 지수			|
-----------------------------------------------------------------
CHAID		| 카이제곱 통계량의 p-value	| ANOVA F-통계량 p-value
-----------------------------------------------------------------




>>> [ 분류 알고리즘 - 의사결정 나무 문제풀이 ] 8:40
Q. 지니지수는 불순도를 측정하는 지수이므로 값이 작을수록 순수도가 높다
Q. 카이제곱 통계량의 p-value는 그 값이 작을수록 자식 노드 내의 불확실성이 큼을 의미한다.
Q. 엔트로피 지수는 p=0.5 일 때 이질성이 가장 크다
Q. 의사결정나무 알고리즘은 하향식 접근 방법을 사용한다.
Q. CHAID는 목표변수가 이산형일 때, Pearson의 카이 제곱 통계량을 분리기준으로 사용한다
Q. CART는 가장 성취도가 좋은 변수 및 수준을 찾는 것에 중점을 둔 알고리즘이다.
Q. 정보이득이란 어떤 속성을 선택함으로 인해서 데이터를 더 잘 구분하게 되는 것을 의미한다.
Q. 선형성, 정규성, 등분산성 등의 수학적 가정이 불필요한 비모수적인 모형이다.




>>> [ 분류 알고리즘 - 앙상블 ] 13:19

* 앙상블 모형
	- 여러 개의 분류 모형에 의한 결과를 종합하여 분류의 정확도를 높이는 방법
	- 적절한 표본추출법으로 데이터에서 여러 대의 훈련용 데이터 집합을 만들어 각 데이터 집합에 하나의 분류기를 만들어 결합하는 방법
	- 약하게 학습 된 여러 모델들을 결합하여 사용
	- 성능을 분산시키기 때문에 과적합 감소 효과가 있음

* 앙상블 모형의 종류
	1) Voting
	2) Bagging
	3) Boosting
	4) Random Forest

* Voting
	- 서로 다른 여러 개 알고리즘 분류기를 사용
	- 각 모델의 결과를 취합하여 많은 결과 또는 높은 확률로 나온 것을 최종 결과로 채택하는 것
	* Hard voting
		- 각 모델의 예측 결과 중 많은 것을 선택
	* Soft voting
		- 각 모델의 클래스 확률을 구하고 평균 높은 확률을 선택

* Bagging ( 배깅, Bootstrap AGGregatING)
	* Bootstrap : 학습 데이터를 준비하는 방법 중 하나 (sampling)
	* AGGregatING : 분류 기준이 여러개 일 때 그 결과를 집계 한다는 뜻
	- 서로 다른 훈련 데이터 샘플로 훈련, 서로 같은 알고리즘 분류기 결합
	- 원 데이터에서 중복을 허용하는 크기가 같은 표본을 여러 번 단순 임의 복원 추출하여 각 표본에 대해 분류기(classifiers)를 생성하는 기법
	- 여러 모델이 병렬로 학습, 그 결과를 집계하는 방식
	- 같은 데이터가 여러번 추출될 수도 있고, 어떤 데이터는 추출되지 않을 수 있음
	- 대표적 알고리즘 : MetaCost Algorithm

* 부스팅(Boosting)
	- 여러 모델이 순차적으로 학습
	- 이전 모델의 결과에 따라 다음 모델 표본 추출에서 분류가 잘못된 데이터에 가중치를 부여하여 표본을 추출함
	- 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있고, 이상치에 약함
	- 대표적 알고리즘 : AdaBoost, GradientBoost(XGBoost, Light GBM) 등
		-> Light GBM : Leaf-wise-node 방법을 사용하는 알고리즘

* 랜덤 포레스트(Random Forest)
	- 배깅에 랜덤 과정을 추가한 방법	
	- 노드 내 데이터를 자식 노드로 나누는 기준을 정할 때 모든 예측변수에서 최적의 분할을 선택하는 대신, 설명변수의 일부부만을 고려함으로 성능을 높이는 방법
	- 여러 개의 의사결정 나무를 사용해, 하나의 나무를 사용할 때보다 과적합 문제를 피할 수 있음

* 시그모이드(sigmoid) : 로지스틱 회귀모델에서 활성화 하는 함수

Q. 상호 연관성이 높을수록 정확도는 감소된다.(분류가 쉽지 않음)

* 랜덤 포레스트 : 앙상블 모델 중 배깅 방식의 하나로 여러 개의 분류기(의사결정트리)를 사용하여 보다 정확한 분류 결과를 낼 수 있는 기법
* 주성분 분석 : 상관관계가 있는 변수들을 결합해 상관관계가 없는 변수로 분산을 극대화하는 변수로 선형결합 해 변수를 축약하는 기법이다.
* 군집 분석 : 레코드 자체가 가진 다른 레코드와의 유사성에 의해 그룹화 하는 기법
* 연관 분석 : '같이 팔리는 물건' 같이 아이템의 연관성을 파악하는 분석




>>> [ 분류 알고리즘 - k-NN, SVM ] 4:08

* k-NN(k-Nearest Neighbors)
	- 새로운 데이터에 대해 주어진 이웃의 개수(k) 만큼 가까운 멤버들과 비교하여 결과를 판단하는 방법
	- k값에 따라 소속되는 그룹이 달라질 수 있음 (k값은 hyper parameter)
	- 거리를 측정해 이웃들을 뽑기 때문에 스케일링이 중요함
	- 반응변수가 범주향이면 분류, 연속형이면 회귀의 목적으로 사용됨
	- 모형을 미리 만들지 않고, 새로운 데이터가 들어오면 그때부터 계산을 시작하는 lazy learning(레이즈 학습)이 사용되는 지도학습 알고리즘

* SVM(Support Vector Machine)
	- 서로 다른 분류에 속한 데이터 간의 간격이 최대가 되는 선을 찾아 이를 기준으로 데이터를 분류하는 모델
	- H3는 분류를 올바르게 하지 못하며, H1, H2는 분류를 올바르게 하는데 더 큰 간격을 갖고 분류하는 H2가 분류 기준이 됨




>>> [ 분류 알고리즘 - 인공신경망 Part1 ] 17:01

* 인공신경망(ANN) 모형
	- 인공신경망을 이용하면 분류 및 예측을 할 수 있음
	- 인공신경망은 입력층, 은닉층, 출력층 3개의 층으로 구성되어 있고, 각 층에 뉴런이 여러개 포함되어 있음
	- 학습 : 입력에 대한 올바른 출력이 나오도록 가중치를 조절하는 것
	- bias, variance : 학습 알고리즘이 갖는 두 가지 종류의 error로, trade off 관계임
		* bias : 지나치게 단순한 모델로 인한 error, bias가 크면 과소 적합을 야기됨
		* variance : 지나치게 복잡한 모델로 인한 error, variace가 크면 과대 적합을 야기함
		* 학습 모형이 유연하다는 것은 복잡도가 증가한다는 것을 의미(bias 낮고, variave 높음)
	=> 인공뉴런(=퍼셉트론)

* 경사하강법 (Gradient descent)
	- 함수 기울기를 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것
	- 제시된 함수의 기울기의 최소값을 찾아내는 머신러닝 알고리즘
	- 비용함수(cost function)을 최소화하기 위해 parameter를 반복적으로 조정하는 과정

* 경사하강법 과정
	1) 임의의 parameter값으로 시작 
	2) cost Function 계산, cost-function - 모델을 구성하는 가중치 w의 함수, 시작점에서 곡선의 기울기 계산
	3) parameter 값 갱신 : W = W - learning rate * 기울기미분값
	=> hyper parameter(learning rate) : 너무 작으면 오랜시간이 걸리고, 너무 크면 발산이 되어서 global minimum을 찾을 수 없음
	4) n번의 반복, 최소값을 향해 수렴함, learning rate가 적절해야 함

* 인공신경망 모형의 장/단점
* 장점 
	- 변수의 수가 많거나 입, 출력변수 간에 복잡한 비선형 관계에 유용
	- 이상치 잡음에 대해서도 민감하게 반응하지 않음
	- 입력변수와 결과변수가 연속형이나 이산형인 경우 모두 처리 가능
* 단점 
	- 결과에 대한 해석이 쉽지 않음
	- 최적의 모형을 도출하는 것이 상대적으로 어려움
	- 모형이 복잡하면 훈련 과정에 시간이 많이 소요됨
	- 데이터를 정규화하지 않으면 지역해(local minimum)에 빠질 위험이 있음
		=> global minimum으로 착각하는 경우가 생김

* 신경망 활성화 함수
	- 결괏값을 내보낼 때 사용하는 함수로, 가중치 값을 학습할 때 에러가 적게 나도록 도움
	- 풀고자 하는 문제 종류에 따라 활성화 함수의 선택이 달라짐
	- 목표 정확도와 학습시간을 고려하여 선택하고 혼합 사용도 함
	- 문제 결과가 직선을 따르는 경향이 있으면 '선형함수'를 사용함

* 활성화 함수의 종류
	- 계단함수 : 0 또는 1 결과
	- 부호함수 : -1 또는 1 결과
	- 선형함수
	- sigmoid : 연속형 0~1, logistic 함수라 불리기도 함, 선형적인 멀티-퍼셉트론에서 비선형 값을 얻기 위해 사용
	- softmax
		1) 모든 logits의 합이 1이 되도록 output을 정규화
		2) sigmoid 함수의 일반화된 형태로 결과가 다 범주인 경우
		각 범주에 속할 사후 확률을 제공하는 활성화 함수
		3) 주로 3개 이상 분류 시 사용함

Q. 분석가의 주관과 경험에 따른다
Q. 입력변수의 속성에 따라 활성화 함수의 선택이 달라진다 X -> 풀고자 하는 문제 종류 O
Q. 역전파 알고리즘이 동일 입력층에 대해 원하는 값이 출력되도록 개개의 weight를 조정하는 방법으로 사용된다.
	


>>> [ 분류 알고리즘 - 인공신경망 Part2 ] 8:48

* 신경망 은닉층, 은닉노드
	- 다층신경망은 단층신경망에 비해 훈련이 어려움
	- 은닉층 수와 은닉 노드 수의 결정은 '분석가가 분석 경험에 의해 설정'함
	
* 은닉층 노드가 너무 적으면
	- 네트워크가 복잡한 의사결정 경계를 만들 수 없음
	- 과소 적합(Underfitting) 문제 발생
* 은닉층 노드가 너무 많으면
	- 복잡성을 잡아낼수 있지만, 일반화가 어렵다
	- 레이어가 많아지면 기울기 소실 문제가 발생할 수 있다.
	- 과적합 문제 발생

* 역전파 알고리즘(Backpropagation Algorithm) 
	- 출력층에서 제시한 값에 대해, 실제 원한느 값으로 학습하는 방법으로 사용
	- 동일 입력층에 대해 원하는 값이 출력되도록 개개의 weight를 조정하는 방법으로 사용됨

* 기울기 소실 문제
	- 다층신경망에서 은닉층이 많아 인공신경망 기울기 값을 베이스로 하는 역전파 알고리즘으로 학습시키려고 할 때 발생하는 문제

* 기울기 소실
	- 역전파 알고리즘은 출력층에서 입력층로 오차 Gradient를 흘려 보내면서, 각 뉴런의 입력 값에 대한 손실함수의 Gradient를 계산함
	- 이렇게 계산한 기울기를 사용하여 각 가중치 매개변수를 업데이터 해줌
	- 다층신경망에서는 역전파 알고리즘이 입력층으로 갈 수록 기울기가 점차적으로 작어져 0으로 수렴하여, wieght가 업데이트 되지 않는 현상
	- activation function(활성화 함수)으로 sigmoid 험수를 사용할 때 발생 -> 해결을 위해 ReLU 등 다른 함수 사용
	=> x=0에서 기울기 최대, x가 크거나 작을 때 기울기가 0에 가까워짐
	=> W = W - a(d/dw)cost(Web)


------------------------------------------------------------------------------------

Number 60 ~ number 76
