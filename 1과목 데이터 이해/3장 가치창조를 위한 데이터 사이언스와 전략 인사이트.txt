# 👩🏻‍💻 1과목 데이터 이해

## 3장. 가치창조를 위한 데이터 사이언스와 전략 인사이트
	01. 빅데이터 분석과 전략 인사이트
	02. 전략 인사이트 도출을 위한 필요한 역량
	03. 빅데이터 그리고 데이터 사이언스의 미래


35~47 numbering object


### 35. 빅데이터의 열풍과 회의론
1) 빅데이터 분석은 데이터에서 가치,
즉 통팔을 끌어내는 것이 성과를 창출하는 것이 관건
-> 복잡하고 다양한 데이터를 최적화 능력이 반드시 최고의 가치를 창출하는 것은 아니고, 가치에 적합한 분석을 하는 것이 중요 포인트

2) 데이터는 크기의 이슈가 아니라, 거기에서 어떤 시각과 통찰을 얻을 수 있느냐의 문제이다. 
빅데이터와 관련된 걸림돌은 비용이 아니라 분석적 방법과 성과에 대한 이해 부족이다.

3) 대부분 성과가 높은 기업일수록 데이터 기반에 의한 의사결정을 하지만 성과가 우수한 기업들도 가치 분석력 통찰력을 갖췄다고 대답한 비율이 낮다.
-> 기업의 핵심가치와 관련한 전략적 통찰력을 가져다 주는 데이터분석을 내재화하는 것이 쉬운 일이 아님



36. 일차적인 분석 vs 전략 도출을 위한 가치 기반분석
1) 일차원적인 분석을 통해서도 해당 부서, 업무영역 효과를 얻을 수 있지만, 일차원적인 분석은 태생적으로 업계 내부의 문제에만 초점을 두고 있음
-> 전략적 인사이트 가치 기반을 위해서 인구통계학적 변화, 경제사회 트랜드, 고객 니즈의 변화 고려해야 함. 
즉, 업계 상황에 한정해서 바라보지 말고 더 넓은 시야에서 차별화를 고려해야 한다.

2) 데이터 분석은 대상을 모델범위 외 요인들을 판단하게 되면 분석 모델의 정확성에 위험을 동반할 수 있음을 유의

* 출제유형
일차원적 분석의 한계점 등이 객관형 오답보기 문제로 출제



37. 데이터 사이언스 vs 데이터 마이닝 vs 통계학 차이
1) 데이터 사이언스란 : 데이터로부터 의미 있는 정보를 추출하는 학문

2) 통계학이 정형화된 실험 데이터를 분석 대상으로 하는 것에 비해, 데이터 사이언스는 정형 또는 비정형을 막론하고 다양한 유형의 데이터를 대상으로 총제적 접근법을 사용 ( 통계학과 차이 )

3) 데이터 마이닝은 주로 분석에 초점 두나, 데이터 사이언스는 분석뿐 아니라 이를 효과적으로 구현하고 전달하는 과정까지 모두 포괄하는 개념 ( 데이터마이닝 차이 )

4) 결국, 데이터 사이언스란 데이터 공학, 수학, 통계학, 컴퓨터 공학, 시각화, 해커의 사고방식, 해당 분야의 전문 지식을 종합한 학문으로 정의



38. 데이터 사이언스 핵심구성요소
1) IT ( Data Management )
2) Analytics ( 분석적 영역 )
3) 비즈니스 분석



39. 데이터 사이언티스트가 갖춰야 할 역량 ( 가트너 )
1) 데이터 관리 : 데이터에 대해 이해
2) 분석 모델링 : 분석론에 대한 지식
3) 비즈니스 분석 : 비즈니스 요소에 초점
4) 소프트 기능 : 커뮤니케이션, 협력, 리더십, 창의력

-> 데이터 사이언티스트의 갖춰야 할 역량의 공통점은 호기심이다.
호기심이란 문제의 이면을 파고들고, 질문들을 찾고, 검증 가능한 가설을 세우는 능력 또한, 스토리텔링, 커뮤니케이션, 직관력, 소통능력 필요

* 출제유형
결국 데이터 사이언티스트는 정량분석이라는 과학과 인문학적 통찰에 근거한 합리적 추론을 조합한다.



40. 데이터 사이언티스트 요구역량 ( 하드스킬 & 소프트스킬 )
1) Hard Skill
	- 빅데이터에 대한 이론적 지식 : 관련 기법에 대한 이해와 방법론 습득
	- 분석 기술에 대한 숙련 : 최적의 분석 설계 및 노하우 축적
	
2) Soft Skill
	- 통찰력 있는 분석 : 창의적 사고, 호기심, 논리적 비판
	- 설득력 있는 전달 : 스토리텔링, Visualization
	- 다문야 간 협력 : Communication



41. 인문학의 부할 이유
1) 단순 세계화에서 복잡한 세계로의 변화 ( convergence -> devergence )
2) 비즈니스의 중심이 제품생산에서 서비스로 이동 ( product -> service )
3) 경제와 산업의 논리가 생산에서 시장 창조



42. 데이터 사이언티스트 6가지 핵심 질문
	| 과거			| 현재			| 미래		
---------------------------------------------------------------------------------
정보	| 무슨 일이 일어났는가? 	| 무슨 일이 일어나고 있는가?	| 무슨 일이 일어날 것인가?
	| -> 리포팅(보고서)		| -> 경고			| -> 추출
통찰	| 어떻게, 왜 일어났는가?	| 차선 행동은 무엇인가?	| 최악, 최선의 상황은?
	| -> 모델링, 실험설계	| -> 권고			| -> 예측, 최적화, 시뮬레이션



43. 가치 패러다임의 변화
1) 디지털화 ( Digitalization ) : 아날로그의 세상을 어떻게 효과적으로 디지털화하는가가 이 시대의 가치를 창출해내는 원천

2) 연결 ( Connection ) : 	디지털화된 정보와 대상들은 서로 연결되어, 이 연결을 얼마나 효과적이고 효율적으로 제공해주느냐가 이 시대의 성패를 결정함

3) 에이전시 ( Agency ) : 사물인터넷(IoT)의 성숙과 함께 연결이 증가하고 연결을 얼마나 효과적이고 믿을 만하게 관리하는가 이슈.
		데이터 사이언스의 역량에 따라 좌우

* 출제유형
가치 패러다임의 변화 순사가 출제



44.자주 출제되는 기타 용어정리
1) 데이터 레이크 ( Data Lake ) : 대규모의 다양한 원시 데이터셋을 기본형식으로 저장하는 데이터 리포지토리 유형. 
데이터 레이크에 있는 데이터는 분석을 위해 필요할 때 변화되며, 이러한 경우 스키마가 적용되어 데이터 분석이 가능해집니다. 
이는 "읽기 스키마(schema on read)"라고 불리는데, 데이터가 사용 준비 상태가 될 때까지 원시 상태로 보관되기 때문 

2) 서비타이제이션( Servitization ) : 제품과 서비스의 결합, 서비스의 상품과 그리고 기존 서비스와 신규 서비스의 결합 협상을 포괄하는 개념

3) 딥러닝 ( Deep Learning ) : 여러 층을 가진 인공신경망을 사용하고 머신러닝 학습을 수행하는 것으로 심층학습기법으로 대표적 분석방법으로 LSTM, Autoencoder, RNN 등이 있다.

4) 마이데이터 : 개인이 각종 기업, 기관에 흩어져 있는 자신의 신용 정보를 마이데이터 사업자에게 활용하도록 하고 이들 업체로부터 자신에게 유용한 맞춤형 서비스를 받는 것을 의미한다.



45. SQL 분류
- SQL ( Structure Query Language )은 관계 데이터베이스를 위한 표준 질의어를 의미한다.
	SQL는 기능에 따라 데이터 정의어(DDL), 데이터 조작어(DML), 데이터 제어어(DCL)로 나눈다

1) 데이터 정의어 ( DDL )
	- 스키마, 테이블, 뷰 등을 정의하거나 변경, 삭제할 때 사용하는 언어
	- 데이터베이스 관리자 혹은 설계자가 사용함
	- 유형 : CREATE / ALTER / DROP

2) 데이터 조작어 ( DML )
	- 데이터베이스 사용자가 저장된 데이터를 처리할 때 사용하는 언어
	- 데이터베이스 사용자가 관리 시스템 간의 인터페이스를 제공함
	- 유형 : SELETE / INSERT / DELETE / UPDATE

3) 데이터 제어어 ( DCL )
	- 데이터의 보안, 무결성 등을 정의하는데 사용하는 언어
	- 데이터베이스 관리자가 데이터 관리 목적으로 사용함
	- 유형 : COMMIT / ROLLBACK / GRANT / REVOKE



46. ETL ( Extraction, Transformation and Load )
- ETL은 데이터 이동과 변환 절차와 관련된 업계표준용어이다.
- ETL은 데이터 웨어하우스( DW ), 운영 데이터 스토어( ODS) , 데이터 마트( DM )에 대한 데이터 적재 작업의 핵심 구성요소이다.
- 데이터 통합(Data Integration), 데이터 이동(Data Migration), 마스트 데이터 관리(MDM, Master Data Management)에 걸쳐 폭넓게 활용된다.
- ETL은 데이터 이동과 변환을 주목적으로 하며 3가지 기능으로 구성된다.

1) Extraction (추출) : 하나 또는 그 이상의 데이터 원천들로부터 데이터 획득
2) Transformation (변형) : 데이터 클렌징, 형식 변환, 통합 또는 다수 애플리케이션 내장된 비즈니스 롤 적용
3) Loading (적재) : 위 변형 단계 처리가 완료된 데이터를 특정 목표 시스템에 적재



47. 하둡의 구성요소
- 하둡은 하나의 성능 좋은 컴퓨터를 이용하여 데이터를 처리하는 대신, 적당한 성능의 범용 컴퓨터 여러 대를 클러스터화하고, 큰 크기의 데이터를 클러스터에서 병렬로 동시에 처리하여 처리 속도를 높이는 것을 목적으로 한다.
- 하둡의 코어프로젝트가 HDFS와 MapReduce이며 이외에도 다양한 서브프로젝트가 존재한다. 이러한 서브 프로젝트들의 모임을 하둡의 에코시스템이라고 한다.

1) HDFS ( Hadoop Distributed File System ) : 네트워크에 연결된 기기에 데이터를 저장하는 분산형 파일 시스템이다.
2) MapReduce : 대용량 데이터 처리를 위한 분산 프로그래밍 모델로, 대규모 분석 컴퓨팅 환경에서 대량의 데이터를 병렬로 분석한다.



